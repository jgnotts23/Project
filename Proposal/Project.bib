Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@unpublished{Lawson2019,
author = {Lawson, Jenna},
file = {:home/jgnotts23/Downloads/Late Stage Review{\_}April{\_}cris{\_}Final.pdf:pdf},
pages = {1--35},
title = {{Sounds of the Spider Monkey : Using acoustics to investigate biodiversity, and land use and threats to the Geoffroy's spider monkey (Ateles geoffroyi) in Costa Rica}},
year = {2019}
}
@article{Yeo2011,
abstract = {Voice recognition systems have become the important applications for speech recognition technology. In this paper, an animal identification (ID) detection system based on animal voice pattern recognition algorithm has been developed. The developed animal voice recognition system uses the zero-cross-rate (ZCR), Mel-Frequency Cepstral Coefficients (MFCC) and Dynamic Time Warping (DTW) joint algorithms as the tools for recognizing the voice of the particular animal. ZCR is used for the end point detection of input voice such that the silence voice can be removed. MFCC is used for the process of feature extraction where a more compact and less redundant of the representative voice can be obtained from the input voice. While the voice pattern classification will be done by using DTW algorithm. The DTW voice pattern classification module is playing a very important role as it is used to get the optimal path between the input voice and the reference voice in the database. The obtained results show that the developed recognition system can be worked as expected.},
author = {Yeo, Che Yong and Al-Haddad, S. A.R. and Ng, Chee Kyun},
doi = {10.1109/CSPA.2011.5759872},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeo, Al-Haddad, Ng - 2011 - Animal voice recognition for identification (ID) detection system.pdf:pdf},
isbn = {9781612844145},
journal = {Proceedings - 2011 IEEE 7th International Colloquium on Signal Processing and Its Applications, CSPA 2011},
keywords = {Animal,DTW,MFCC,Voice Recognition,ZCR},
number = {Id},
pages = {198--201},
title = {{Animal voice recognition for identification (ID) detection system}},
year = {2011}
}
@article{Sueur2018,
abstract = {R topics document},
author = {Sueur, J{\'{e}}r{\^{o}}me and Aubin, Thierry and Simonis, Caroline and Lellouch, Laurent and Brown, Ethan C. and Depraetere, Marion and Desjonqueres, Camille and Fabianek, Francois and Gasc, Amandine and Kasten, Eric and LaZerte, Stefanie and Lees, Jonathan and Marchal, Jean and Pavoine, Sandrine and Pinaud, David and Stotz, Alicia and Villanueva-Rivera, Luis J. and Ross, Zev and Witthoft, Carl G. and Zhivomirov, Hristo},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sueur et al. - 2018 - Sound Analysis and Synthesis, Package 'seewave'.pdf:pdf},
keywords = {2D and 3D spec-trograms and many other analyses Li,Alicia Stotz [ctrb],Amandine Gasc [ctrb],Author Jerome Sueur {\textless}sueur@mnhnfr{\textgreater} [cre,Camille Desjonqueres [ctrb],Carl G Witthoft [ctrb],Caroline Simonis [au],David Pinaud [ctrb],Eric Kasten [ctrb],Ethan C Brown [ctrb],Francois Fabianek [ctrb],Hristo Zhivomirov [ctrb] Maintainer Jerome Sueur {\textless},Jean Marchal [ctrb],Jonathan Lees [ctrb],Laurent Lellouch [main ctrb],Luis J Villanueva-Rivera [ctrb],Marion Depraetere [ctrb],Sandrine Pavoine [ctrb],Stefanie LaZerte [ctrb],Thierry Aubin [au],Zev Ross [ctrb],analytic signal,au],cross correlation and autocorrela-tion,displaying,dominant frequency,editing and synthesiz-ing time waves (particularly,entropy,fftw,frequency coherence,ggplot2,grDevices,manipulating,phonTools,resonance quality factor,rgl,rpanel,signal ZipData no Description Functions for analys,spectral content,stats,tuneR Suggests audio,utils,zero-crossing},
pages = {201},
title = {{Sound Analysis and Synthesis, Package 'seewave'}},
url = {http://rug.mnhn.fr/seewave},
year = {2018}
}
@article{Sueur2008,
abstract = {ABSTRACT We review Seewave, new software for analysing and synthesizing sounds. Seewave is free and works on a wide variety of operating systems as an extension of the R operating environment. Its current 67 functions allow the user to achieve time, amplitude and frequency analyses, to estimate quantitative differences between sounds, and to generate new sounds for playback experiments. Thanks to its implementation in the R environment, Seewave is fully modular. All functions can be combined for complex data acquisition and graphical output, they can be part of important scripts for batch processing and they can be modified ad libitum. New functions can also be written, making Seewave a truly open-source tool.},
author = {Sueur, J and Aubin, T and Simons, C},
doi = {10.1080/09524622.2008.9753600},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sueur, Aubin, Simons - 2008 - Seewave, a Free Modular Tool for Sound Analysis and Synthesis.pdf:pdf},
isbn = {3527312102},
issn = {03771237},
journal = {Medical Journal Armed Forces India},
number = {3},
pages = {241},
title = {{Seewave, a Free Modular Tool for Sound Analysis and Synthesis}},
volume = {58},
year = {2008}
}
@article{Mielke2013,
author = {Mielke, Alexander and Zuberb{\"{u}}hler, Klaus},
doi = {10.1016/j.anbehav.2013.04.017},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mielke, Zuberb{\"{u}}hler - 2013 - A method for automated individual, species and call type recognition in free-ranging animals.pdf:pdf},
issn = {0003-3472},
journal = {Animal Behaviour},
number = {2},
pages = {475--482},
publisher = {Elsevier Ltd},
title = {{A method for automated individual, species and call type recognition in free-ranging animals}},
url = {http://dx.doi.org/10.1016/j.anbehav.2013.04.017},
volume = {86},
year = {2013}
}
@article{Snaddon2017,
author = {Snaddon, Jake L. and Hill, Andrew P. and Prince, Peter and Rogers, Alex and {Pi{\~{n}}a Covarrubias}, Evelyn and Doncaster, C. Patrick},
doi = {10.1111/2041-210x.12955},
file = {:home/jgnotts23/Downloads/AudioMoth- Evaluation of a smart open acoustic device for monitoring biodiversity and the environment.pdf:pdf},
journal = {Methods in Ecology and Evolution},
number = {5},
pages = {1199--1211},
title = {{AudioMoth: Evaluation of a smart open acoustic device for monitoring biodiversity and the environment}},
volume = {9},
year = {2017}
}
@article{Accepts2008,
author = {Accepts, J V I and Society, American and Authors, Listed and Reserved, All Rights},
doi = {10.1128/JVI.00721-08},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Accepts et al. - 2008 - 强光胁迫下外源 N O对霍山石斛叶绿素荧光和抗氧 化系统的影响.pdf:pdf},
journal = {Microbiology},
number = {June},
title = {{强光胁迫下外源 N O对霍山石斛叶绿素荧光和抗氧 化系统的影响}},
volume = {4},
year = {2008}
}
@article{Clarke2010,
abstract = {The loud songs of gibbons (Hylobatidae) usually consist of a duet by the mated pair delivered each morning. These songs can transmit over a kilometre through dense forest habitat and therefore presumably play a role in long-distance communication. There is some evidence to suggest that gibbons use song in contexts other than their daily duets, such as predation, but these songs have not been well studied. Close- range communication is also relevant for gibbons, but these quieter calls have completely escaped any detailed observation. The responses of wild white-handed gibbons (Hylobates lar) to simulated visual and acoustic predators (tiger, clouded leopard, reticulated python and crested serpent eagle) were studied in Khao Yai National Park, Thailand to address the lack of empirical data about these important events. Little is known about gibbons' anti- predatory behaviour in general, and simulated predator encounters provided an opportunity to investigate these responses as well. Results showed that gibbons used song as part of their anti-predator strategy and that subtle combinatorial changes were meaningful to conspecifics. They also showed marked behavioural changes in the short-term, and some evidence of longer-term changes as well. Quiet calls were also part of the gibbons' response repertoire with the hoo call being particularly relevant. Hoos were used as a prelude to singing both normal duets and predator songs, but there were consistent differences between each context. Hoos were also delivered independently in a number of other contexts outside predation. When analysed, these hoos showed consistent contextual differences in a number of spectral parameters. Within the duet context, important contextual subtleties were evident also revealing a remarkable vocal plasticity. In addition, gibbons voluntarily attended to specific vocal elements of other gibbon duets, indicating that certain sequences are more pertinent than others. Results suggest both gibbon song and gibbon hoos are powerful communication tools that reliably reference external objects and events; this ability is also a critical feature of human language.},
author = {Clarke, Esther Anne Elizabeth},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clarke - 2010 - The Vocalisations and Anti-predatory Behaviour of Wild White-handed Gibbons ( Hylobates lar ) in Khao Yai National Park.pdf:pdf},
pages = {1--233},
title = {{The Vocalisations and Anti-predatory Behaviour of Wild White-handed Gibbons ( Hylobates lar ) in Khao Yai National Park , Thailand}},
year = {2010}
}
@article{Clark1993,
author = {Clark, Adam P and Wrangham, Richard W},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clark, Wrangham - 1993 - Acoustic Analysis of Wild Chimpanzee Pant Hoots Do Kibale Forest Chimpanzees Have an Acoustically Distinct Foo.pdf:pdf},
journal = {American Journal of Primatology},
keywords = {acoustic analysis,chimpanzee,pant hoot,semanticity},
pages = {99--109},
title = {{Acoustic Analysis of Wild Chimpanzee Pant Hoots : Do Kibale Forest Chimpanzees Have an Acoustically Distinct Food Arrival Pant Hoot?}},
volume = {31},
year = {1993}
}
@article{Tang2009,
abstract = {There are two important research topics in the field of Music Information Retrieval (MIR). One is how to improve the robustness of features and the other is how to speed up the retrieval process. This paper improved the algorithms which proposed by Shazam company in these two aspects. We improve the robustness of the system by a new audio finger-printing extraction using computer graphics, and the system can recognize the recordings which get in complex environment accurately. On the other hand, we propose a recursive search algorithm based on the confidence measure to improve the retrieval speed. Quantitative analysis of the opposite experiment verifies the improvement in the retrieval speed and accuracy.},
author = {Tang, Jie and Liu, Gang and Guo, Jun},
doi = {10.1109/IITAW.2009.110},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tang, Liu, Guo - 2009 - Improved algorithms of music information retrieval based on audio fingerprint.pdf:pdf},
isbn = {9780769538600},
journal = {3rd International Symposium on Intelligent Information Technology Application Workshops, IITAW 2009},
keywords = {Audio fingerprinting,Computer vision,Recursive retrieval},
pages = {367--371},
title = {{Improved algorithms of music information retrieval based on audio fingerprint}},
year = {2009}
}
@article{Hill2018,
abstract = {Introduction: D-dimer assay, generally evaluated according to cutoff points calibrated for VTE exclusion, is used to estimate the individual risk of recurrence after a first idiopathic event of venous thromboembolism (VTE). Methods: Commercial D-dimer assays, evaluated according to predetermined cutoff levels for each assay, specific for age (lower in subjects {\textless}70 years) and gender (lower in males), were used in the recent DULCIS study. The present analysis compared the results obtained in the DULCIS with those that might have been had using the following different cutoff criteria: traditional cutoff for VTE exclusion, higher levels in subjects aged ≥60 years, or age multiplied by 10. Results: In young subjects, the DULCIS low cutoff levels resulted in half the recurrent events that would have occurred using the other criteria. In elderly patients, the DULCIS results were similar to those calculated for the two age-adjusted criteria. The adoption of traditional VTE exclusion criteria would have led to positive results in the large majority of elderly subjects, without a significant reduction in the rate of recurrent event. Conclusion: The results confirm the usefulness of the cutoff levels used in DULCIS.},
author = {Hill, Andrew P. and Prince, Peter and {Pi{\~{n}}a Covarrubias}, Evelyn and Doncaster, C. Patrick and Snaddon, Jake L. and Rogers, Alex},
doi = {10.1111/2041-210X.12955},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hill et al. - 2018 - AudioMoth Evaluation of a smart open acoustic device for monitoring biodiversity and the environment.pdf:pdf},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {acoustic monitoring,biodiversity monitoring,ecosystem management,gunshot detection,open science,open-source hardware,open-source software},
number = {5},
pages = {1--13},
title = {{AudioMoth: Evaluation of a smart open acoustic device for monitoring biodiversity and the environment}},
volume = {9},
year = {2018}
}
@article{Breebaart2013,
abstract = {Four audio feature sets are evaluated in their ability to differentiate five audio classes: pop- ular music, classical music, speech, noise and crowd noise. The feature sets include low-level signal properties, mel-frequency spectral coefficients, and two new sets based on perceptual models of hearing. The temporal behavior of the features is analyzed and parameterized and these parameters are included as additional features. Using a standard Gaussian framework for classification, results show that the temporal behavior of features is important for automatic audio classification. In addition, classification is better, on average, if based on features from models of auditory perception rather than on standard features.},
author = {Breebaart, Jeroen and McKinney, Martin F.},
doi = {10.1007/978-94-017-0703-9_6},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Breebaart, McKinney - 2013 - Features for Audio Classification.pdf:pdf},
isbn = {9789401707039},
number = {October},
pages = {113--129},
title = {{Features for Audio Classification}},
year = {2013}
}
@article{Effects2015,
abstract = {Our understanding of the epidemiology, risk factors and natural history of pneumocystis pneumonia has increased enormously since the first patients with AIDS were described. It is clear that the major risk factor for developing pneumocystis pneumonia is immunosuppression as reflected by a circulating CD4 lymphocyte count of less than 200/$\mu$l. However, recent analyses indicate that persons with more than 200 CD4 cells may be at increased risk if they have symptomatic HIV disease. Thus, such patients are also candidates for prophylaxis. Also of note is the fact that whites have a higher risk of pneumocystis pneumonia than blacks. The use of prophylactic regimens clearly reduces the incidence of pneumocystis pneumonia as demonstrated by the results under 'real-world' conditions. The main affect appears to be a delay in the onset of the disease until the immune compromise is much more severe. Pneumocystis pneumonia rarely presents without respiratory symptoms-cough and/or shortness of breath. Although constitutional symptoms are present commonly, they are in themselves only rarely indicative of pneumocystis pneumonia. The most common radiographic feature of pneumocystis pneumonia is diffuse 'ground glass' infiltration. However, a variety of findings may be noted. These include focal infiltration, cysts, cavities, pneumothorax and miliary infiltration. Severe pneumocystis pneumonia may result in respiratory failure that necessitates mechanical ventilation. Over the years the prognosis for patients who require mechanical ventilation because of pneumocystis pneumonia has varied. In general, however, fewer patients seem to be requiring mechanical support. Other complications of the disease include chronic pneumothorax and chronic airways obstruction. Both of these conditions can be very difficult to manage.},
author = {Effects, Digital Audio},
doi = {10.13140/RG.2.1.1471.4640},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Effects - 2015 - AN EVALUATION OF AUDIO FEATURE EXTRACTION TOOLBOXES David Moffat , David Ronan , Joshua D . Reiss Center for Digital Mu.pdf:pdf},
issn = {1071-6564},
pages = {1--7},
title = {{AN EVALUATION OF AUDIO FEATURE EXTRACTION TOOLBOXES David Moffat , David Ronan , Joshua D . Reiss Center for Digital Music Queen Mary University of London Mile End Road}},
year = {2015}
}
@article{Ramos-Fernandez2010,
abstract = {Spider monkeys are one of the most widespread New World primate genera, ranging from southern Mexico to Bolivia. Although they are common in zoos, spider monkeys are traditionally very difficult to study in the wild, because they are fast moving, live high in the canopy and are almost always found in small subgroups that vary in size and composition throughout the day. The past decade has seen an expansion in research being carried out on this genus and this book is an assimilation of both published, and new and previously unpublished, research. It is a comprehensive source of information for academic researchers and graduate students interested in primatology, evolutionary anthropology and behavioral ecology and covers topics such as taxonomy, diet, sexuality and reproduction, and conservation.},
author = {Ramos-Fern{\'{a}}ndez, Gabriel},
doi = {10.1017/cbo9780511721915.008},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramos-Fern{\'{a}}ndez - 2010 - Communication in spider monkeys the function and mechanisms underlying the use of the whinny.pdf:pdf},
isbn = {9780511721915},
journal = {Spider Monkeys},
number = {January},
pages = {220--235},
title = {{Communication in spider monkeys: the function and mechanisms underlying the use of the whinny}},
year = {2010}
}
@article{Mohamed2010,
author = {Mohamed, Abdel-rahman and Dahl, George E and Hinton, Geoffrey},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohamed, Dahl, Hinton - 2010 - Acoustic Modeling using Deep Belief Networks.pdf:pdf},
journal = {IEEE Signal Processing Letters},
pages = {1--10},
title = {{Acoustic Modeling using Deep Belief Networks}},
year = {2010}
}
@article{Reason2016,
abstract = {Robust information on bat species distribution and activity is lacking. With developments in passive full spectrum bat detectors and software packages for automating the analysis of sound files, there is the potential to analyse large volumes of acoustic data and thus inform a better understanding of bat ecology and distribution. However, for anyone making use of such tools, it is essential to understand the limitations and likely biases of the software, in order to make an informed interpretation. Bat activity Defining bat activity (as recorded by detectors) Relative bat activity can be measured from the search-phase echolocation calls of bats or, more commonly, from 'bat passes/sequences' – where a pass/sequence is a series of calls belonging to an individual bat. • Unless manually determined by a human observer, the way bat calls are divided into bat passes is defined by the automatic identification (Auto-ID) software selected. • 'Bat pass' is therefore a parameter that needs to be defined for each study. • The number of bat calls or bat passes does not directly relate to the number of bats in a location, although recent statistical approaches may now be able to model this directly 1 . It is important to consider carefully (and possibly ground truth) how relative bat activity will be interpreted. Defining bat passes • Bat passes are often defined by a minimum number of calls in a series and/or the time between calls or a series of calls (the inter-pulse interval). • For example, a bat pass could be any call or series of calls separated by more than one second from another call or series of calls, although other definitions state that a bat pass comprises at least two calls in a series. • The definition of bat passes therefore needs to be determined before analysis, clearly stated, and kept consistent throughout a project. • Some software (e.g. iBatsID using SonoBat-generated parameters) classifies each call rather than a series of calls, so further processing is required to determine the number of bat passes and to evaluate the most likely species based on how these are assigned over the entire series.},
author = {Reason, Paola F. and Newson, Stuart E and Jones, Kate E.},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reason, Newson, Jones - 2016 - Recommendations for using automatic bat identification software with full spectrum recordings.pdf:pdf},
journal = {Bat Conservation Trust},
keywords = {automatic identification,bats,full spectrum},
number = {April},
pages = {1--4},
title = {{Recommendations for using automatic bat identification software with full spectrum recordings}},
year = {2016}
}
@article{Fristrup2012,
author = {Fristrup, Kurt M and Mennitt, Dan},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fristrup, Mennitt - 2012 - terrestrial environments . Hornworts.pdf:pdf},
journal = {History},
number = {July},
pages = {738--739},
title = {{terrestrial environments . Hornworts}},
volume = {8},
year = {2012}
}
@book{Russ2013,
author = {Russ, Jon},
edition = {1},
publisher = {Pelagic Publishing},
title = {{British Bat Calls: A Guide to Species Identification}},
year = {2013}
}
@article{Fischer2001,
author = {Fischer, Julia and Hammerschmidt, Kurt and Cheney, Dorothy L and Seyfarth, Robert M},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer et al. - 2001 - Acoustic Features of Female Chacma Baboon Barks.pdf:pdf},
journal = {Ethology},
pages = {33--54},
title = {{Acoustic Features of Female Chacma Baboon Barks}},
volume = {107},
year = {2001}
}
@article{Gillespie2013,
abstract = {PAMGUARD is open‐source, platform‐independent software to address the needs of developers and users of Passive Acoustic Monitoring (PAM) systems. For the PAM operator—marine mammal biologist, manager, or mitigator—PAMGUARD provides a flexible and easy‐to‐use suite of detection, localization, data management, and display modules. These provide a standard interface across different platforms with the flexibility to allow multiple detectors to be added, removed, and configured according to the species of interest and the hardware configuration on a particular project. For developers of PAM systems, an Application Programming Interface (API) has been developed which contains standard classes for the efficient handling of many types of data, interfaces to acquisition hardware and to databases, and a GUI framework for data display. PAMGUARD replicates and exceeds the capabilities of earlier real time monitoring programs such as the IFAW Logger Suite and Ishmael. Ongoing developments include improved real‐time location and automated species classification.},
author = {Gillespie, Douglas and Mellinger, David K. and Gordon, Jonathan and McLaren, David and Redmond, Paul and McHugh, Ronald and Trinder, Philip and Deng, Xiao‐Yan and Thode, Aaron},
doi = {10.1121/1.4808713},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gillespie et al. - 2013 - PAMGUARD Semiautomated, open source software for real‐time acoustic detection and localization of cetaceans.pdf:pdf},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {2547--2547},
title = {{PAMGUARD: Semiautomated, open source software for real‐time acoustic detection and localization of cetaceans.}},
volume = {125},
year = {2013}
}
@book{.2554,
author = {{นคเรศ รังควัต.}},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/.pdf:pdf},
isbn = {9780499284105},
title = {{No Titleกระบวนการสื่อสารกับการยอมรับปรัชญาเศรษฐกิจพอเพียงของเกษตรกร ในจังหวัดเชียงใหม่}},
year = {2554}
}
@article{Edwards2017,
author = {Edwards, Bryn},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Edwards - 2017 - Using bioacoustics to assess the impacts of forest fire on primates .pdf:pdf},
number = {September},
title = {{Using bioacoustics to assess the impacts of forest fire on primates .}},
year = {2017}
}
@article{Norouzzadeh2017,
abstract = {Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would revolutionize our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could transform many fields of biology, ecology, and zoology into "big data" sciences. Motion sensor "camera traps" enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2-million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with over 93.8{\%} accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3{\%} of the data while still performing at the same 96.6{\%} accuracy as that of crowdsourced teams of human volunteers, saving more than 8.4 years (at 40 hours per week) of human labeling effort (i.e. over 17,000 hours) on this 3.2-million-image dataset. Those efficiency gains immediately highlight the importance of using deep neural networks to automate data extraction from camera-trap images. Our results suggest that this technology could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.},
archivePrefix = {arXiv},
arxivId = {1703.05830},
author = {Norouzzadeh, Mohammed Sadegh and Nguyen, Anh and Kosmala, Margaret and Swanson, Ali and Palmer, Meredith and Packer, Craig and Clune, Jeff},
doi = {10.1073/pnas.1719367115},
eprint = {1703.05830},
file = {:home/jgnotts23/Downloads/E5716.full.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
number = {25},
pages = {716--725},
title = {{Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning}},
volume = {115},
year = {2018}
}
@article{Prince2019,
abstract = {{\textless}p{\textgreater}Conservation researchers require low-cost access to acoustic monitoring technology. However, affordable tools are often constrained to short-term studies due to high energy consumption and limited storage. To enable long-term monitoring, energy and space efficiency must be improved on such tools. This paper describes the development and deployment of three acoustic detection algorithms that reduce the power and storage requirements of acoustic monitoring on affordable, open-source hardware. The algorithms aim to detect bat echolocation, to search for evidence of an endangered cicada species, and also to collect evidence of poaching in a protected nature reserve. The algorithms are designed to run on AudioMoth: a low-cost, open-source acoustic monitoring device, developed by the authors and widely adopted by the conservation community. Each algorithm addresses a detection task of increasing complexity, implementing extra analytical steps to account for environmental conditions such as wind, analysing samples multiple times to prevent missed events, and incorporating a hidden Markov model for sample classification in both the time and frequency domain. For each algorithm, we report on real-world deployments carried out with partner organisations and also benchmark the hidden Markov model against a convolutional neural network, a deep-learning technique commonly used for acoustics. The deployments demonstrate how acoustic detection algorithms extend the use of low-cost, open-source hardware and facilitate a new avenue for conservation researchers to perform large-scale monitoring.{\textless}/p{\textgreater}},
author = {Prince, Peter and Hill, Andrew and {Pi{\~{n}}a Covarrubias}, Evelyn and Doncaster, Patrick and Snaddon, Jake L and Rogers, Alex},
doi = {10.3390/s19030553},
issn = {14248220},
journal = {Sensors (Basel, Switzerland)},
keywords = {acoustics,bioacoustics,conservation,ecology,machine learning},
number = {3},
pages = {1--23},
title = {{Deploying Acoustic Detection Algorithms on Low-Cost, Open-Source Acoustic Sensors for Environmental Monitoring}},
volume = {19},
year = {2019}
}
@article{Zamora-Gutierrez2016,
abstract = {* Monitoring global biodiversity is critical for understanding responses to anthropogenic change, but biodiversity monitoring is often biased away from tropical, megadiverse areas that are experiencing more rapid environmental change. Acoustic surveys are increasingly used to monitor biodiversity change, especially for bats as they are important indicator species and most use sound to detect, localise and classify objects. However, using bat acoustic surveys for monitoring poses several challenges, particularly in megadiverse regions. Many species lack reference recordings, some species have high call similarity or differ in call detectability, and quantitative classification tools, such as machine learning algorithms, have rarely been applied to data from these areas. * Here, we collate a reference call library for bat species that occur in a megadiverse country, Mexico. We use 4685 search-phase calls from 1378 individual sequences of 59 bat species to create automatic species identification tools generated by machine learning algorithms (Random Forest). We evaluate the improvement in species-level classification rates gained by using hierarchical classifications, reflecting either taxonomic or ecological constraints (guilds) on call design, and examine how classification rate accuracy changes at different hierarchical levels (family, genus and guild). * Species-level classification of calls had a mean accuracy of 66{\%}, and the use of hierarchies improved mean species-level classification accuracy by up to 6{\%} (species within families 72{\%}, species within genera 71{\textperiodcentered}2{\%} and species within guilds 69{\textperiodcentered}1{\%}). Classification accuracy to family, genus and guild-level was 91{\textperiodcentered}7{\%}, 77{\textperiodcentered}8{\%} and 82{\textperiodcentered}5{\%}, respectively. * The bioacoustic identification tools we have developed are accurate for rapid biodiversity assessments in a megadiverse region and can also be used effectively to classify species at broader taxonomic or ecological levels. This flexibility increases their usefulness when there are incomplete species reference recordings and also offers the opportunity to characterise and track changes in bat community structure. Our results show that bat bioacoustic surveys in megadiverse countries have more potential than previously thought to monitor biodiversity changes and can be used to direct further developments of bioacoustic monitoring programs in Mexico.},
author = {Zamora-Gutierrez, Veronica and Lopez-Gonzalez, Celia and {MacSwiney Gonzalez}, M. Cristina and Fenton, Brock and Jones, Gareth and Kalko, Elisabeth K.V. and Puechmaille, Sebastien J. and Stathopoulos, Vassilios and Jones, Kate E.},
doi = {10.1111/2041-210X.12556},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zamora-Gutierrez et al. - 2016 - Acoustic identification of Mexican bats based on taxonomic and ecological constraints on call design.pdf:pdf},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Neotropical,acoustic identification,guild,hierarchical classification,machine learning,random forest,whispering bats},
number = {9},
pages = {1082--1091},
title = {{Acoustic identification of Mexican bats based on taxonomic and ecological constraints on call design}},
volume = {7},
year = {2016}
}
@article{Merchant2015,
abstract = {1. Many organisms depend on sound for communication, predator/prey detection and navigation. The acoustic environment can therefore play an important role in ecosystem dynamics and evolution. A growing number of studies are documenting acoustic habitats and their influences on animal development, behaviour, physiology and spatial ecology, which has led to increasing demand for passive acoustic monitoring (PAM) expertise in the life sciences. However, as yet, there has been no synthesis of data processing methods for acoustic habitat monitoring, which presents an unnecessary obstacle to would-be PAM analysts. 2. Here, we review the signal processing techniques needed to produce calibrated measurements of terrestrial and aquatic acoustic habitats. We include a supplemental tutorial and template computer codes in matlab and r, which give detailed guidance on how to produce calibrated spectrograms and statistical analyses of sound levels. Key metrics and terminology for the characterisation of biotic, abiotic and anthropogenic sound are covered, and their application to relevant monitoring scenarios is illustrated through example data sets. To inform study design and hardware selection, we also include an up-to-date overview of terrestrial and aquatic PAM instruments. 3. Monitoring of acoustic habitats at large spatiotemporal scales is becoming possible through recent advances in PAM technology. This will enhance our understanding of the role of sound in the spatial ecology of acoustically sensitive species and inform spatial planning to mitigate the rising influence of anthropogenic noise in these ecosystems. As we demonstrate in this work, progress in these areas will depend upon the application of consistent and appropriate PAM methodologies.},
author = {Merchant, Nathan D. and Fristrup, Kurt M. and Johnson, Mark P. and Tyack, Peter L. and Witt, Matthew J. and Blondel, Philippe and Parks, Susan E.},
doi = {10.1111/2041-210X.12330},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Merchant et al. - 2015 - Measuring acoustic habitats.pdf:pdf},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Acoustic ecology,Ambient noise,Anthropogenic noise,Bioacoustics,Ecoacoustics,Habitat monitoring,Passive acoustic monitoring,Remote sensing,Soundscape},
number = {3},
pages = {257--265},
title = {{Measuring acoustic habitats}},
volume = {6},
year = {2015}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning （PPT).pdf:pdf},
isbn = {9780521835688},
issn = {0028-0836},
journal = {Nature 2015},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning （PPT)}},
url = {http://dx.doi.org/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Fischer2003,
author = {Fischer, Julia and Hammerschmidt, Kurt and Cheney, Dorothy L and Seyfarth, Robert M},
doi = {10.1121/1.1433807},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer et al. - 2003 - Acoustic features of male baboon loud calls Influences of context, age, and individuality.pdf:pdf},
journal = {The Journal of the Acoustical Society of America},
number = {1465-1474},
title = {{Acoustic features of male baboon loud calls: Influences of context, age, and individuality}},
volume = {111},
year = {2003}
}
@article{Walters2012,
abstract = {1. Acoustic methods are used increasingly to survey and monitor bat populations. However, the use of acoustic methods at continental scales can be hampered by the lack of standardized and objective methods to identify all species recorded. This makes comparable continent-wide monitoring difficult, impeding progress towards developing biodiversity indicators, transboundary conservation programmes and monitoring species distribution changes. 2. Here we developed a continental-scale classifier for acoustic identification of bats, which can be used throughout Europe to ensure objective, consistent and comparable species identifications. We selected 1350 full-spectrum reference calls from a set of 15 858 calls of 34 European species, from EchoBank, a global echolocation call library. We assessed 24 call parameters to evaluate how well they distinguish between species and used the 12 most useful to train a hierarchy of ensembles of artificial neural networks to distinguish the echolocation calls of these bat species. 3. Calls are first classified to one of five call-type groups, with a median accuracy of 976{\%}. The median species-level classification accuracy is 837{\%}, providing robust classification for most European species, and an estimate of classification error for each species. 4. These classifiers were packaged into an online tool, iBatsID, which is freely available, enabling anyone to classify European calls in an objective and consistent way, allowing standardized acoustic identification across the continent. 5. Synthesis and applications. iBatsID is the first freely available and easily accessible continental- scale bat call classifier, providing the basis for standardized, continental acoustic bat monitoring in Europe. This method can provide key information to managers and conservation planners on distribution changes and changes in bat species activity through time.},
author = {Walters, Charlotte L. and Freeman, Robin and Collen, Alanna and Dietz, Christian and {Brock Fenton}, M. and Jones, Gareth and Obrist, Martin K. and Puechmaille, S{\'{e}}bastien J. and Sattler, Thomas and Siemers, Bj{\"{o}}rn M. and Parsons, Stuart and Jones, Kate E.},
doi = {10.1111/j.1365-2664.2012.02182.x},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Walters et al. - 2012 - A continental-scale tool for acoustic identification of European bats.pdf:pdf},
issn = {00218901},
journal = {Journal of Applied Ecology},
keywords = {Acoustic monitoring,Chiroptera,Classification,Echolocation,Ensembles of artificial neural networks,Indicator species,Species identification},
number = {5},
pages = {1064--1074},
title = {{A continental-scale tool for acoustic identification of European bats}},
volume = {49},
year = {2012}
}
@article{Giannakopoulos2015,
abstract = {Audio information plays a rather important role in the increasing digital content that is available today, resulting in a need for methodologies that automatically analyze such content: audio event recognition for home automations and surveillance systems, speech recognition, music information retrieval, multimodal analysis (e.g. audio-visual analysis of online videos for content-based recommendation), etc. This paper presents pyAudioAnalysis, an open-source Python library that provides a wide range of audio analysis procedures including: feature extraction, classification of audio signals, supervised and unsupervised segmentation and content visualization. pyAudioAnalysis is licensed under the Apache License and is available at GitHub (https://github.com/tyiannak/pyAudioAnalysis/). Here we present the theoretical background behind the wide range of the implemented methodologies, along with evaluation metrics for some of the methods. pyAudioAnalysis has been already used in several audio analysis research applications: smart-home functionalities through audio event detection, speech emotion recognition, depression classification based on audio-visual features, music segmentation, multimodal content-based movie recommendation and health applications (e.g. monitoring eating habits). The feedback provided from all these particular audio applications has led to practical enhancement of the library.},
author = {Giannakopoulos, Theodoros},
doi = {10.1371/journal.pone.0144610},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Giannakopoulos - 2015 - PyAudioAnalysis An open-source python library for audio signal analysis.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {12},
pages = {1--17},
title = {{PyAudioAnalysis: An open-source python library for audio signal analysis}},
url = {http://dx.doi.org/10.1371/journal.pone.0144610},
volume = {10},
year = {2015}
}
@article{Kalan2015,
author = {Kalan, Ammie K and Mundry, Roger and Wagner, Oliver J J and Heinicke, Stefanie and Boesch, Christophe and K{\"{u}}hl, Hjalmar S},
doi = {10.1016/j.ecolind.2015.02.023},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalan et al. - 2015 - Towards the automated detection and occupancy estimation of primates using passive acoustic monitoring.pdf:pdf},
issn = {1470-160X},
journal = {Ecological Indicators},
pages = {217--226},
publisher = {Elsevier Ltd},
title = {{Towards the automated detection and occupancy estimation of primates using passive acoustic monitoring}},
volume = {54},
year = {2015}
}
@article{Prince2019,
abstract = {{\textless}p{\textgreater}Conservation researchers require low-cost access to acoustic monitoring technology. However, affordable tools are often constrained to short-term studies due to high energy consumption and limited storage. To enable long-term monitoring, energy and space efficiency must be improved on such tools. This paper describes the development and deployment of three acoustic detection algorithms that reduce the power and storage requirements of acoustic monitoring on affordable, open-source hardware. The algorithms aim to detect bat echolocation, to search for evidence of an endangered cicada species, and also to collect evidence of poaching in a protected nature reserve. The algorithms are designed to run on AudioMoth: a low-cost, open-source acoustic monitoring device, developed by the authors and widely adopted by the conservation community. Each algorithm addresses a detection task of increasing complexity, implementing extra analytical steps to account for environmental conditions such as wind, analysing samples multiple times to prevent missed events, and incorporating a hidden Markov model for sample classification in both the time and frequency domain. For each algorithm, we report on real-world deployments carried out with partner organisations and also benchmark the hidden Markov model against a convolutional neural network, a deep-learning technique commonly used for acoustics. The deployments demonstrate how acoustic detection algorithms extend the use of low-cost, open-source hardware and facilitate a new avenue for conservation researchers to perform large-scale monitoring.{\textless}/p{\textgreater}},
author = {Prince, Peter and Hill, Andrew and {Pi{\~{n}}a Covarrubias}, Evelyn and Doncaster, Patrick and Snaddon, Jake L. and Rogers, Alex},
doi = {10.3390/s19030553},
file = {:home/jgnotts23/Downloads/sensors-19-00553.pdf:pdf},
issn = {14248220},
journal = {Sensors (Basel, Switzerland)},
keywords = {acoustics,bioacoustics,conservation,ecology,machine learning},
number = {3},
pages = {1--23},
title = {{Deploying Acoustic Detection Algorithms on Low-Cost, Open-Source Acoustic Sensors for Environmental Monitoring}},
volume = {19},
year = {2019}
}
@article{Bello2014,
author = {Bello, Juan Pablo},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bello - 2014 - Sound Classification.pdf:pdf},
number = {April 2013},
pages = {37--41},
title = {{Sound Classification}},
year = {2014}
}
@article{Digby2013,
author = {Digby, Andrew and Towsey, Michael and Bell, Ben D and Teal, Paul D},
doi = {10.1111/2041-210X.12060},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Digby et al. - 2013 - A practical comparison of manual and autonomous methods for acoustic monitoring.pdf:pdf},
journal = {Methods},
pages = {675--683},
title = {{A practical comparison of manual and autonomous methods for acoustic monitoring}},
volume = {4},
year = {2013}
}
@article{Sueur2018a,
author = {Sueur, Jerome},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sueur - 2018 - IO of sound with R (seewave).pdf:pdf},
pages = {1--8},
title = {{I/O of sound with R (seewave)}},
year = {2018}
}
@article{Lucas2015,
author = {Lucas, Tim C D and Moorcroft, Elizabeth A and Freeman, Robin and Rowcliffe, J Marcus and Jones, Kate E},
doi = {10.1111/2041-210X.12346},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lucas et al. - 2015 - A generalised random encounter model for estimating animal density with remote sensor data.pdf:pdf},
journal = {Methods in Ecology and Evolution},
pages = {500--509},
title = {{A generalised random encounter model for estimating animal density with remote sensor data}},
volume = {6},
year = {2015}
}
@article{Stevenson2015,
author = {Stevenson, Ben C and Borchers, David L and Altwegg, Res and Swift, J and Gillespie, Douglas M and Measey, G John},
doi = {10.1111/2041-210X.12291},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stevenson et al. - 2015 - A general framework for animal density estimation from acoustic detections across a fixed microphone array.pdf:pdf},
journal = {Methods in Ecology and Evolution},
pages = {38--48},
title = {{A general framework for animal density estimation from acoustic detections across a fixed microphone array}},
volume = {6},
year = {2015}
}
@article{Snaddon2017,
author = {Snaddon, Jake L and Hill, Andrew P and Prince, Peter and Rogers, Alex and {Pi{\~{n}}a Covarrubias}, Evelyn and Doncaster, C Patrick},
doi = {10.1111/2041-210x.12955},
journal = {Methods in Ecology and Evolution},
number = {5},
pages = {1199--1211},
title = {{AudioMoth: Evaluation of a smart open acoustic device for monitoring biodiversity and the environment}},
volume = {9},
year = {2017}
}
@book{Bradbury2011,
author = {Bradbury, Jack and Vehrencamp, Sandra},
edition = {2},
publisher = {OUP, USA},
title = {{Principles of Animal Communication}},
year = {2011}
}
@article{Steen2012,
author = {Steen, Kim Arild and Therkildsen, Ole Roland and Karstoft, Henrik and Green, Ole},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Steen et al. - 2012 - Wildlife Communication.pdf:pdf},
number = {January},
title = {{Wildlife Communication}},
year = {2012}
}
@article{Zeppelzauer2010,
author = {Zeppelzauer, Matthias and Breiteneder, Christian},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeppelzauer, Breiteneder - 2010 - Mitrović{\_}2010a{\_}Features for content-based audio retrieval.pdf.pdf:pdf},
pages = {71--150},
title = {{Mitrović{\_}2010a{\_}Features for content-based audio retrieval.pdf}},
volume = {78},
year = {2010}
}
@article{Lecun2015,
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lecun, Bengio, Hinton - 2015 - DeepLearning{\_}2015{\_}YannLeCunnYoshuaBengioGeoffreyHinton.pdf:pdf},
title = {{DeepLearning{\_}2015{\_}YannLeCunnYoshuaBengioGeoffreyHinton}},
year = {2015}
}
@techreport{Browning2017,
abstract = {Wrege, as well as 8 participants who chose to remain anonymous. Funding for this report was provided by WWF-UK. WWF is one of the world's largest and most experienced independent conservation organizations, with over 5 million supporters and a global network active in more than 100 countries. WWF's mission is to stop the degredation of the planets natural environment and to build a future in which humans live in harmony with nature by conserving the worlds biological diversity, ensuring that the use of renewable natural resources is sustainable, and promoting the reduction of pollution and wasteful consumption. Cover Image: Transient Orca (Orcinus orca) calls recorded in Glacier Bay, Alaska {\textcopyright} V. Deecke},
author = {Browning, Ella and Gibb, Rory and Glover-Kapfer, Paul and Jones, Kate E.},
booktitle = {WWF Conservation Technology Series 1(2)},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Browning et al. - 2017 - Passive acoustic monitoring in ecology and conservation.pdf:pdf},
issn = {0964-6906},
pages = {1--75},
pmid = {11875046},
title = {{Passive acoustic monitoring in ecology and conservation}},
volume = {2},
year = {2017}
}
@article{Camastra2009,
abstract = {One of the most interesting technological phenomena in recent years is the diffusion of consumer electronic products with constantly increasing acquisition, storage and processing power. As an example, consider the evolution of digital cameras: the first models available in the market in the early nineties produced images composed of 1.6 million pixels (this is the meaning of the expression 1.6 megapixels), carried an onboard memory of 16megabytes, and had an average cost higher than 10,000 U.S. dollars. At the time this book is being written, the best models are close to or even above 8 megapixels, have internal memories of one gigabyte and they cost around 1,000 U.S. dollars. In other words, while resolution and memory capacity have been multiplied by around five and fifty, respectively, the price has been divided by more than ten. Similar trends can be observed in all other kinds of digital devices including videocameras, cellular phones, mp3 players, personal digital assistants (PDA), etc. As a result, large amounts of digital material are being accumulated and need to be managed effectively in order to avoid the problem of information overload.},
author = {Camastra, Francesco},
doi = {10.1117/1.3152242},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Camastra - 2009 - Machine Learning for Audio, Image and Video Analysis.pdf:pdf},
issn = {1017-9909},
journal = {Journal of Electronic Imaging},
number = {2},
pages = {029901},
title = {{Machine Learning for Audio, Image and Video Analysis}},
volume = {18},
year = {2009}
}
@article{Goeau2016,
abstract = {The LifeCLEF bird identification challenge provides a large-scale testbed for the system-oriented evaluation of bird species identifi-cation based on audio recordings. One of its main strength is that the data used for the evaluation is collected through Xeno-Canto, the largest network of bird sound recordists in the world. This makes the task closer to the conditions of a real-world application than previous, similar initia-tives. The main novelty of the 2016-th edition of the challenge was the inclusion of soundscape recordings in addition to the usual xeno-canto recordings that focus on a single foreground species. This paper reports the methodology of the conducted evaluation, the overview of the sys-tems experimented by the 6 participating research groups and a synthetic analysis of the obtained results.},
author = {Go{\"{e}}au, Herv{\'{e}} and Glotin, Herv{\'{e}} and Vellinga, Willem-Pier and Planqu{\'{e}}, Robert and Joly, Alexis},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Go{\"{e}}au et al. - 2016 - LifeCLEF Bird Identification Task 2016 The arrival of Deep learning.pdf:pdf},
journal = {Working Notes of CLEF 2016 - Conference and Labs of the Evaluation forum},
keywords = {LifeCLEF,audio,benchmark,bioacoustics,bird,call,collection,eco-logical monitoring,evaluation,fine-grained classification,iden-tification,retrieval,song,species},
pages = {440--449},
title = {{LifeCLEF Bird Identification Task 2016: The arrival of Deep learning.}},
url = {https://hal.archives-ouvertes.fr/hal-01373779/document},
year = {2016}
}
@phdthesis{Butler2018,
author = {Butler, Duncan},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Butler - 2018 - Creating a deep-learning automated audio detection system for Geoffroy ' s spider monkey , Ateles geoffroyi(2).pdf:pdf},
pages = {1--22},
school = {Imperial College London},
title = {{Creating a deep-learning automated audio detection system for Geoffroy ' s spider monkey , Ateles geoffroyi}},
year = {2018}
}
@article{Cartwright2017,
abstract = {Audio annotation is key to developing machine-listening systems; yet, effective ways to accurately and rapidly obtain crowdsourced audio annotations is understudied. In this work, we seek to quantify the relia-bility/redundancy trade-off in crowdsourced soundscape annotation, investigate how visualizations affect accuracy and efficiency, and characterize how performance varies as a function of audio characteristics. Using a controlled experiment, we varied sound visualizations and the complexity of soundscapes presented to human annotators. Results show that more complex audio scenes result in lower annotator agreement, and spectrogram visualizations are superior in producing higher quality annotations at lower cost of time and human labor. We also found recall is more affected than precision by soundscape complexity, and mistakes can be often attributed to certain sound event characteristics. These findings have implications not only for how we should design annotation tasks and interfaces for audio data, but also how we train and evaluate machine-listening systems.},
author = {Cartwright, Mark and Salamon, Justin and Williams, Alex and Mikloska, Stefanie and Macconnell, Duncan and Bello, Juan P and Seals, Ayanna and Law, Edith and Nov, Oded and Law, ; Edith},
doi = {10.1145/3134664},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartwright et al. - 2017 - Seeing Sound Investigating the Effects of Visualizations and Complexity on Crowdsourced Audio Annotations.pdf:pdf},
journal = {Proc. ACM Hum.-Comput. Interact. 1, CSCW, Article},
keywords = {Annotation,Sound Event Detection},
number = {2},
pages = {29},
title = {{Seeing Sound: Investigating the Effects of Visualizations and Complexity on Crowdsourced Audio Annotations}},
volume = {29},
year = {2017}
}
@article{Owren1993,
author = {Owren, Michael J and Linker, Christopher D and Rowe, Matthew P},
doi = {10.1121/1.407791},
file = {:home/jgnotts23/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Owren, Linker, Rowe - 1993 - Acoustic features of tonal ‘‘grunt'' calls in baboons.pdf:pdf},
journal = {The Journal of the Acoustical Society of America},
pages = {1822--1823},
title = {{Acoustic features of tonal ‘‘grunt'' calls in baboons}},
volume = {94},
year = {1993}
}
@article{Salamon2017,
abstract = {The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a "shallow" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1608.04363v2},
author = {Salamon, Justin and Bello, Juan Pablo},
doi = {10.1109/LSP.2017.2657381},
eprint = {arXiv:1608.04363v2},
file = {:home/jgnotts23/Downloads/1608.04363.pdf:pdf},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Deep convolutional neural networks (CNNs),deep learning,environmental sound classification,urban sound dataset},
number = {3},
pages = {279--283},
title = {{Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification}},
volume = {24},
year = {2017}
}
